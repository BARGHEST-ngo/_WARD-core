"""
MemoryExploitationHeuristic 

Detects episode-based heap dump/leak indicators using temporal grouping, deduplication,
and zero-click correlation to reduce noise and highlight post-exploitation artifacts.

Key Features:
- Episode-based temporal grouping of memory events
- Zero-click vs one-click detection methodology
- Media processing vulnerability detection (codecs, decoders, image processing)
- Corroborator analysis (HPROF files, OOM, GC thrashing, log wipes)
- Post-exploitation artifact correlation

Zero-Click Detection:
- Background parser/sandbox behavior (not name-based)
- No foreground activity in timing window (-5s to +3s)
- Memory safety violation signals present
- Score boost: 1.5x multiplier

One-Click Detection:
- Activity start detected before crash (-3s to 0s)
- App was topmost/foreground
- Score dampening: 0.8x multiplier
"""

import re
from collections import defaultdict
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from ward_core.logic.models import LogData, Detection, Evidence, EvidenceType, Severity
from ward_core.heuristics.base import BaseHeuristic


class EpisodeType(Enum):
    """Types of memory exploitation episodes."""
    BACKTRACE_ONLY = "backtrace_only"
    SUSPECTED_HEAP_DUMP = "suspected_heap_dump"
    EXPLOIT_CRASH = "exploit_crash"


class DumpKind(Enum):
    """Types of memory dumps detected."""
    HPROF = "hprof"
    NATIVE_HEAP = "native_heap"
    JAVA_HEAP = "java_heap"
    DEBUGGERD = "debuggerd"


@dataclass
class Corroborators:
    """Corroborating evidence for memory exploitation episodes."""
    has_hprof_file: bool = False
    has_oom_text: bool = False
    has_gc_thrash: bool = False
    has_implant_artifact: bool = False
    has_log_wipe: bool = False
    has_decoder_hint: bool = False
    has_fatal_signal: bool = False

    # KERNEL EXPLOITATION CORROBORATORS (2025 Android Security)
    has_binder_corruption: bool = False
    has_gpu_driver_crash: bool = False
    has_perf_event_abuse: bool = False
    has_futex_exploitation: bool = False
    has_kernel_oops: bool = False


@dataclass
class MemoryEpisode:
    """A grouped episode of related memory exploitation events."""
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    process: Optional[str] = None
    pid: Optional[str] = None
    episode_type: EpisodeType = EpisodeType.BACKTRACE_ONLY
    dump_kind: Optional[DumpKind] = None
    raw_lines: List[str] = None
    corroborators: Corroborators = None
    
    def __post_init__(self):
        if self.raw_lines is None:
            self.raw_lines = []
        if self.corroborators is None:
            self.corroborators = Corroborators()


class MemoryExploitationHeuristic(BaseHeuristic):
    """Advanced memory exploitation detection with zero-click/one-click methodology."""
    
    # Core heap dump indicators
    HEAP_DUMP_PATTERNS = [
        # Actual memory-related patterns from real ADB logs and tombstones

        # Native crash tombstone patterns (most important for exploitation)
        re.compile(r'signal 11 \(SIGSEGV\)', re.IGNORECASE),
        re.compile(r'signal 6 \(SIGABRT\)', re.IGNORECASE),
        re.compile(r'signal 7 \(SIGBUS\)', re.IGNORECASE),
        re.compile(r'Build fingerprint:', re.IGNORECASE),
        re.compile(r'Abort message:', re.IGNORECASE),
        re.compile(r'fault addr', re.IGNORECASE),
        re.compile(r'backtrace:', re.IGNORECASE),

        # Actual Java heap issues (realistic patterns)
        re.compile(r'java\.lang\.OutOfMemoryError', re.IGNORECASE),
        re.compile(r'Failed to allocate.*bytes', re.IGNORECASE),
        re.compile(r'OutOfMemoryError.*heap', re.IGNORECASE),

        # Native memory allocation failures
        re.compile(r'malloc.*failed', re.IGNORECASE),
        re.compile(r'mmap.*failed', re.IGNORECASE),
        re.compile(r'Unable to allocate', re.IGNORECASE),

        # Process crashes from ActivityManager (common in logcat)
        re.compile(r'ActivityManager.*died', re.IGNORECASE),
        re.compile(r'Process.*died.*signal', re.IGNORECASE),
        re.compile(r'FATAL EXCEPTION', re.IGNORECASE),

        # Tombstone file references in logcat
        re.compile(r'Tombstone written to:', re.IGNORECASE),
        re.compile(r'DEBUG.*\*\*\* \*\*\* \*\*\*', re.IGNORECASE),

        # WebView crashes (high-value zero-click targets)
        re.compile(r'webview.*crash', re.IGNORECASE),
        re.compile(r'chrome.*sandboxed.*process.*crash', re.IGNORECASE),
        re.compile(r'renderer.*crash', re.IGNORECASE),

        # Media framework crashes (zero-click attack surface)
        re.compile(r'media.*server.*died', re.IGNORECASE),
        re.compile(r'stagefright.*crash', re.IGNORECASE),
        re.compile(r'codec.*crash', re.IGNORECASE),

        # System service crashes
        re.compile(r'system_server.*crash', re.IGNORECASE),
        re.compile(r'zygote.*crash', re.IGNORECASE),
        re.compile(r'surfaceflinger.*died', re.IGNORECASE),

    ]

    # REALISTIC KERNEL/SYSTEM PATTERNS from actual Android logs
    KERNEL_PATTERNS = [
        # Actual kernel messages from dmesg/logcat
        re.compile(r'Kernel panic', re.IGNORECASE),
        re.compile(r'kernel BUG at', re.IGNORECASE),
        re.compile(r'Oops:', re.IGNORECASE),
        re.compile(r'Unable to handle kernel', re.IGNORECASE),

        # System service crashes (realistic patterns)
        re.compile(r'system_server.*died', re.IGNORECASE),
        re.compile(r'zygote.*died', re.IGNORECASE),
        re.compile(r'surfaceflinger.*died', re.IGNORECASE),
        re.compile(r'mediaserver.*died', re.IGNORECASE),

        # Binder issues (actual patterns from logs)
        re.compile(r'binder.*transaction.*too large', re.IGNORECASE),
        re.compile(r'binder.*died', re.IGNORECASE),
        re.compile(r'binder.*failed.*reply', re.IGNORECASE),

        # Low memory killer (realistic system behavior)
        re.compile(r'lowmemorykiller.*Killing', re.IGNORECASE),
        re.compile(r'Out of memory.*Kill process', re.IGNORECASE),

        # Watchdog timeouts (system instability)
        re.compile(r'Watchdog.*timeout', re.IGNORECASE),
        re.compile(r'ANR.*system_server', re.IGNORECASE),
    ]
    
    # Filter out boilerplate debuggerd noise
    BOILERPLATE_PATTERNS = [
        re.compile(r"Wrote stack traces to '\[tombstoned\]'", re.IGNORECASE),
        re.compile(r'libdebuggerd_client: done dumping process', re.IGNORECASE),
        re.compile(r'dumpstate: Excluding stale dump file', re.IGNORECASE),
    ]
    
    # Background parser/sandbox processes (zero-click targets)
    BACKGROUND_PARSER_PROCESSES = [
        'stagefright', 'mediaserver', 'mediadrmserver', 'mediacodec',
        'imagedecodert', 'skia', 'libwebp', 'ccodec', 'webview_zygote',
        'com.android.media', 'media.codec', 'media.swcodec'
    ]

    # KERNEL-LEVEL PROCESSES (2025 Android Security - zero-click targets)
    KERNEL_LEVEL_PROCESSES = [
        'binder', 'kgsl-3d0', 'perf', 'futex', 'kernel',
        'system_server', 'surfaceflinger', 'audioserver'
    ]
    
    # High-risk processes for surface analysis
    HIGH_RISK_PROCESSES = [
        'media', 'codec', 'decoder', 'renderer', 'camera', 'audio',
        'stagefright', 'skia', 'webview', 'chrome'
    ]
    
    # Memory safety violation signals (enhanced with FORTIFY/scudo + kernel exploits)
    STRONG_MEMORY_SIGNALS = [
        # Actual tombstone signal patterns (what we see in real data)
        re.compile(r'signal 11 \(SIGSEGV\)', re.IGNORECASE),
        re.compile(r'signal 6 \(SIGABRT\)', re.IGNORECASE),
        re.compile(r'signal 7 \(SIGBUS\)', re.IGNORECASE),
        re.compile(r'signal 4 \(SIGILL\)', re.IGNORECASE),

        # Fault address patterns from tombstones
        re.compile(r'fault addr 0x0+\b', re.IGNORECASE),  # NULL pointer dereference
        re.compile(r'fault addr 0x[0-9a-f]+', re.IGNORECASE),  # Memory access violations

        # Signal codes that indicate memory corruption
        re.compile(r'code 128 \(SI_KERNEL\)', re.IGNORECASE),  # Kernel-generated signal
        re.compile(r'code -6 \(SI_TKILL\)', re.IGNORECASE),   # Thread kill (often from abort)
        re.compile(r'code 1 \(SEGV_MAPERR\)', re.IGNORECASE), # Address not mapped
        re.compile(r'code 2 \(SEGV_ACCERR\)', re.IGNORECASE), # Invalid permissions

        # Abort messages from actual crashes
        re.compile(r'Abort message:', re.IGNORECASE),
        re.compile(r'JNI FatalError called:', re.IGNORECASE),
        re.compile(r'Fatal signal', re.IGNORECASE),

        # Build fingerprint indicates tombstone file
        re.compile(r'Build fingerprint:', re.IGNORECASE),

        # Tombstone header pattern
        re.compile(r'\*\*\* \*\*\* \*\*\* \*\*\* \*\*\* \*\*\* \*\*\* \*\*\*', re.IGNORECASE),

        # Process crash patterns from logcat
        re.compile(r'FATAL EXCEPTION:', re.IGNORECASE),
        re.compile(r'AndroidRuntime.*FATAL EXCEPTION', re.IGNORECASE),

        # Native crash patterns in logcat
        re.compile(r'DEBUG.*\*\*\* \*\*\* \*\*\*', re.IGNORECASE),
        re.compile(r'DEBUG.*Build fingerprint', re.IGNORECASE),
        re.compile(r'DEBUG.*Tombstone written to:', re.IGNORECASE),

        # Actual memory-related crash patterns
        re.compile(r'libc.*Fatal signal', re.IGNORECASE),
        re.compile(r'Unable to start activity.*RuntimeException', re.IGNORECASE),
    ]
    
    WEAK_MEMORY_SIGNALS = [
        # Generic signal mentions (less specific than STRONG patterns)
        re.compile(r'signal.*11', re.IGNORECASE),
        re.compile(r'signal.*6', re.IGNORECASE),
        re.compile(r'signal.*7', re.IGNORECASE),

        # Process death patterns from ActivityManager
        re.compile(r'ActivityManager.*died', re.IGNORECASE),
        re.compile(r'Process.*died', re.IGNORECASE),
        re.compile(r'Killing.*\(adj', re.IGNORECASE),

        # Generic crash indicators
        re.compile(r'backtrace:', re.IGNORECASE),
        re.compile(r'stack:', re.IGNORECASE),
        re.compile(r'memory near', re.IGNORECASE),
        re.compile(r'code around', re.IGNORECASE),

        # Register dump indicators
        re.compile(r'rax [0-9a-f]+.*rbx [0-9a-f]+', re.IGNORECASE),  # x86_64 registers
        re.compile(r'eax [0-9a-f]+.*ebx [0-9a-f]+', re.IGNORECASE),  # x86 registers
        re.compile(r'r[0-9]+ [0-9a-f]+', re.IGNORECASE),             # ARM registers
    ]
    
    # Foreground activity patterns
    ACTIVITY_START_PATTERNS = [
        re.compile(r'ActivityManager.*START.*intent', re.IGNORECASE),
        re.compile(r'ActivityManager.*Displayed.*activity', re.IGNORECASE),
        re.compile(r'Activity.*onCreate|onStart|onResume', re.IGNORECASE),
        re.compile(r'WindowManager.*addWindow', re.IGNORECASE),
    ]
    
    # User interaction patterns
    USER_INTERACTION_PATTERNS = [
        re.compile(r'Input.*event.*ACTION_DOWN', re.IGNORECASE),
        re.compile(r'Touch.*event', re.IGNORECASE),
        re.compile(r'Key.*event.*ACTION_DOWN', re.IGNORECASE),
        re.compile(r'User.*interaction', re.IGNORECASE),
    ]

    def __init__(self, config: Dict[str, Any] = None):
        super().__init__(config)

        # Initialize shared services (local import to avoid circular dependency)
        from logic.services.episode_service import EpisodeService
        from logic.services.crash_analysis_service import CrashAnalysisService

        self.episode_service = EpisodeService()
        self.crash_service = CrashAnalysisService(self.episode_service)

        self.max_detections_per_category = 10
        self.episode_window_seconds = 15  # Extended for media decode chains
        self.zero_click_window_before = 5  # [-5s, +3s] as per spec
        self.zero_click_window_after = 3
        self.zero_click_boost_multiplier = 1.5
        self.one_click_dampener_multiplier = 0.8
        self.min_alert_threshold = 4.0  # Raised threshold to reduce false positives, still not sure on this

        # Performance optimization: timestamp cache with line index
        self._timestamp_cache = {}  # (line_index, line_content) -> timestamp mapping

    @property
    def name(self) -> str:
        return "memory_exploitation"

    @property
    def category(self) -> str:
        return "Memory Exploitation"

    @property
    def description(self) -> str:
        return "Detects memory exploitation attempts using episode-based analysis with zero-click/one-click methodology"

    def analyze(self, log_data: LogData) -> List[Detection]:
        """Main analysis function implementing episode-based memory exploitation detection."""
        detections = []

        # Debug logging to understand what data we're receiving
        self.logger.info(f"Memory exploitation analysis: Processing {len(log_data.raw_lines)} raw lines")
        self.logger.info(f"Memory exploitation analysis: Processing {len(log_data.parsed_events)} parsed events")

        # Check for tombstone-related content in parsed events
        tombstone_events = [e for e in log_data.parsed_events if 'tombstone' in e.get('entry_type', '').lower()]
        self.logger.info(f"Memory exploitation analysis: Found {len(tombstone_events)} tombstone events")

        # Check for raw tombstone files that weren't parsed
        raw_tombstone_events = [e for e in log_data.parsed_events
                               if e.get('entry_type') == 'raw_file' and 'tombstone' in e.get('parsed_content', {}).get('filename', '').lower()]
        self.logger.info(f"Memory exploitation analysis: Found {len(raw_tombstone_events)} raw tombstone files")

        # Check for signal-related content in raw lines
        signal_lines = [line for line in log_data.raw_lines if 'signal' in line.lower() and ('sigsegv' in line.lower() or 'sigabrt' in line.lower())]
        self.logger.info(f"Memory exploitation analysis: Found {len(signal_lines)} signal lines")

        # Process raw tombstone files if they exist
        if raw_tombstone_events:
            self.logger.info("Processing raw tombstone files for memory exploitation indicators")
            for event in raw_tombstone_events:
                content = event.get('parsed_content', {}).get('content', '')
                filename = event.get('parsed_content', {}).get('filename', '')
                self.logger.info(f"Processing raw tombstone: {filename}")

                # Add tombstone content to raw_lines for processing
                if content:
                    tombstone_lines = content.split('\n')
                    log_data.raw_lines.extend(tombstone_lines)
                    self.logger.info(f"Added {len(tombstone_lines)} lines from {filename} to analysis")

        # Step 1: Detect heap dump indicators
        heap_lines = self._detect_heap_dump_indicators(log_data.raw_lines)
        self.logger.info(f"Memory exploitation analysis: Found {len(heap_lines)} heap dump indicators")
        if not heap_lines:
            return detections
        
        # Step 2: Group into episodes using temporal windows
        episodes = self._group_into_episodes(heap_lines, log_data)
        if not episodes:
            return detections
        
        # Step 3: Deduplicate and merge related episodes using shared service
        episodes = self.episode_service.deduplicate_items(
            episodes,
            get_key=lambda e: f"{e.process}_{e.pid}",
            merge_func=self._merge_episodes
        )

        # Step 3.5: Check for repeated crashes (same UID within time window)
        repeated_crashes = self._detect_repeated_crashes(episodes)

        # Step 4: Analyze each episode for exploitation indicators
        for episode in episodes:
            # Classify episode evidence and type
            self._classify_episode_evidence(episode)
            self._classify_episode_type(episode)
            
            # Calculate base score
            base_score = self._calculate_base_score(episode, log_data)
            if base_score < 2.0:  # Skip low-scoring episodes
                continue

            # Apply zero-click boost or one-click dampening
            final_score, methodology_evidence = self._apply_click_methodology(episode, log_data, base_score)

            # Check for repeated crash pattern
            if episode.process in repeated_crashes:
                final_score += 1.5  # Boost for repeated crashes
                methodology_evidence.append(f"Repeated crashes detected: {repeated_crashes[episode.process]} occurrences")

            # Create detection if score meets raised threshold and has multiple indicators
            if final_score >= self.min_alert_threshold and self._has_multiple_indicators(episode):
                detection = self._create_episode_detection(episode, final_score, methodology_evidence)
                detections.append(detection)
                
                if len(detections) >= self.max_detections_per_category:
                    break
        
        return detections

    def _detect_heap_dump_indicators(self, lines: List[str]) -> List[Tuple[int, str]]:
        """Detect lines that indicate heap dump activity with line numbers."""
        heap_lines = []
        
        for i, line in enumerate(lines):
            if not line or not isinstance(line, str):
                continue
                
            # Check for heap dump patterns
            for pattern in self.HEAP_DUMP_PATTERNS:
                if pattern.search(line):
                    # Filter out boilerplate noise
                    is_boilerplate = any(bp.search(line) for bp in self.BOILERPLATE_PATTERNS)
                    if not is_boilerplate:
                        heap_lines.append((i, line))
                        break
        
        return heap_lines

    def _group_into_episodes(self, heap_lines: List[Tuple[int, str]], log_data: LogData) -> List[MemoryEpisode]:
        """Group related heap dump events into episodes using temporal windows."""
        if not heap_lines:
            return []
        
        episodes = []
        current_episode = None
        
        # Sort lines by timestamp for proper temporal grouping
        timestamped_lines = []
        for line_num, line in heap_lines:
            timestamp = self._extract_timestamp(line, line_num, log_data)
            if timestamp:
                timestamped_lines.append((timestamp, line_num, line))

        timestamped_lines.sort(key=lambda x: x[0])  # Sort by timestamp

        for timestamp, line_num, line in timestamped_lines:
            process = self._extract_process_name(line)
            pid = self._extract_pid(line)

            # Check if this line belongs to current episode
            # Enhanced grouping: same process AND (same PID OR within time window)
            belongs_to_current = False
            if current_episode and current_episode.end_time:
                # Compare against end_time to prevent splitting continuous episodes
                time_diff = (timestamp - current_episode.end_time).total_seconds()

                # Same process is required
                if process and current_episode.process and process == current_episode.process:
                    # PID continuity (preferred) or time window from last event (fallback)
                    if (pid and current_episode.pid and pid == current_episode.pid) or \
                       (time_diff <= self.episode_window_seconds):
                        belongs_to_current = True

            if belongs_to_current:
                # Add to current episode
                current_episode.raw_lines.append(line)
                current_episode.end_time = timestamp
                if pid and not current_episode.pid:
                    current_episode.pid = pid
            else:
                # Start new episode
                if current_episode:
                    episodes.append(current_episode)

                current_episode = MemoryEpisode(
                    start_time=timestamp,
                    end_time=timestamp,
                    process=process,
                    pid=pid,
                    raw_lines=[line]
                )

        # Add final episode
        if current_episode:
            episodes.append(current_episode)

        # Merge similar episodes manually (MemoryEpisode is incompatible with generic episode service)
        merged_episodes = self._merge_similar_memory_episodes(episodes)

        return merged_episodes

    def _merge_similar_memory_episodes(self, episodes: List[MemoryEpisode]) -> List[MemoryEpisode]:
        """Merge similar memory episodes that are close in time and process."""
        if len(episodes) <= 1:
            return episodes

        merged = []
        used_indices = set()

        for i, episode in enumerate(episodes):
            if i in used_indices:
                continue

            # Find similar episodes to merge
            similar_episodes = [episode]
            used_indices.add(i)

            for j, other_episode in enumerate(episodes[i+1:], i+1):
                if j in used_indices:
                    continue

                # Check if episodes are similar (same process, close in time)
                if (episode.process == other_episode.process and
                    episode.start_time and other_episode.start_time and
                    abs((episode.start_time - other_episode.start_time).total_seconds()) <= 30):
                    similar_episodes.append(other_episode)
                    used_indices.add(j)

            # Merge similar episodes
            if len(similar_episodes) > 1:
                merged_episode = similar_episodes[0]
                for other_episode in similar_episodes[1:]:
                    merged_episode = self._merge_episodes(merged_episode, other_episode)
                merged.append(merged_episode)
            else:
                merged.append(episode)

        return merged

    def _detect_repeated_crashes(self, episodes: List[MemoryEpisode]) -> Dict[str, int]:
        """Detect repeated crashes for same process within time window."""
        crash_counts = defaultdict(list)

        # Group crashes by process
        for episode in episodes:
            if episode.process and episode.start_time:
                crash_counts[episode.process].append(episode.start_time)

        # Find processes with multiple crashes within 10-15 minutes
        repeated_crashes = {}
        for process, timestamps in crash_counts.items():
            if len(timestamps) >= 2:
                timestamps.sort()
                # Check if crashes occurred within 15 minutes of each other
                for i in range(len(timestamps) - 1):
                    time_diff = (timestamps[i + 1] - timestamps[i]).total_seconds()
                    if time_diff <= 900:  # 15 minutes
                        repeated_crashes[process] = len(timestamps)
                        break

        return repeated_crashes

    def _merge_episodes(self, episode1: MemoryEpisode, episode2: MemoryEpisode) -> MemoryEpisode:
        """Merge two episodes into one, preserving all evidence and corroborators."""
        # Merge corroborators using OR logic to preserve all evidence
        merged_corroborators = Corroborators()
        if episode1.corroborators and episode2.corroborators:
            # Merge only the attributes that actually exist in Corroborators class
            merged_corroborators.has_hprof_file = episode1.corroborators.has_hprof_file or episode2.corroborators.has_hprof_file
            merged_corroborators.has_oom_text = episode1.corroborators.has_oom_text or episode2.corroborators.has_oom_text
            merged_corroborators.has_gc_thrash = episode1.corroborators.has_gc_thrash or episode2.corroborators.has_gc_thrash
            merged_corroborators.has_implant_artifact = episode1.corroborators.has_implant_artifact or episode2.corroborators.has_implant_artifact
            merged_corroborators.has_log_wipe = episode1.corroborators.has_log_wipe or episode2.corroborators.has_log_wipe
            merged_corroborators.has_decoder_hint = episode1.corroborators.has_decoder_hint or episode2.corroborators.has_decoder_hint
            merged_corroborators.has_fatal_signal = episode1.corroborators.has_fatal_signal or episode2.corroborators.has_fatal_signal

            # Kernel exploitation corroborators
            merged_corroborators.has_binder_corruption = episode1.corroborators.has_binder_corruption or episode2.corroborators.has_binder_corruption
            merged_corroborators.has_gpu_driver_crash = episode1.corroborators.has_gpu_driver_crash or episode2.corroborators.has_gpu_driver_crash
            merged_corroborators.has_perf_event_abuse = episode1.corroborators.has_perf_event_abuse or episode2.corroborators.has_perf_event_abuse
            merged_corroborators.has_futex_exploitation = episode1.corroborators.has_futex_exploitation or episode2.corroborators.has_futex_exploitation
            merged_corroborators.has_kernel_oops = episode1.corroborators.has_kernel_oops or episode2.corroborators.has_kernel_oops
        elif episode1.corroborators:
            merged_corroborators = episode1.corroborators
        elif episode2.corroborators:
            merged_corroborators = episode2.corroborators

        # Choose the most severe episode type
        episode_type = episode1.episode_type
        if episode2.episode_type.value > episode1.episode_type.value:
            episode_type = episode2.episode_type

        # Merge dump kinds - prefer the more specific one
        dump_kind = episode1.dump_kind or episode2.dump_kind

        merged = MemoryEpisode(
            start_time=min(episode1.start_time or datetime.now(), episode2.start_time or datetime.now()),
            end_time=max(episode1.end_time or datetime.now(), episode2.end_time or datetime.now()),
            process=episode1.process or episode2.process,
            pid=episode1.pid or episode2.pid,
            episode_type=episode_type,
            dump_kind=dump_kind,
            raw_lines=episode1.raw_lines + episode2.raw_lines,
            corroborators=merged_corroborators
        )
        return merged

    def _classify_episode_evidence(self, episode: MemoryEpisode):
        """Classify evidence within the episode to set corroborators."""
        for line in episode.raw_lines:
            line_lower = line.lower()
            
            # Check for HPROF files (demote diagnostic dumps)
            if '.hprof' in line_lower or 'hprof-conv' in line_lower:
                # Demote if it's a manual/diagnostic dump
                if 'am dumpheap' in line_lower or 'debug.dumpheap' in line_lower:
                    # Manual dump - lower suspicion, don't set flag
                    pass
                else:
                    episode.corroborators.has_hprof_file = True
            
            # Check for OOM text
            if 'outofmemoryerror' in line_lower or 'failed to allocate' in line_lower:
                episode.corroborators.has_oom_text = True
            
            # Check for GC thrashing
            if 'gc_for_alloc' in line_lower and 'heavy' in line_lower:
                episode.corroborators.has_gc_thrash = True
            
            # Check for decoder hints (zero-click indicators)
            if any(decoder in line_lower for decoder in ['decoder', 'codec', 'media', 'stagefright']):
                episode.corroborators.has_decoder_hint = True
            
            # Check for fatal signals
            if 'fatal signal' in line_lower or 'sigsegv' in line_lower or 'sigbus' in line_lower:
                episode.corroborators.has_fatal_signal = True

            # KERNEL EXPLOITATION CORROBORATOR DETECTION (2025 Android Security)

            # Binder IPC corruption indicators
            if any(pattern in line_lower for pattern in ['binder transaction failed', 'binder died unexpectedly',
                                                        'binder buffer error', 'binder refcount']):
                episode.corroborators.has_binder_corruption = True

            # GPU driver crash indicators
            if any(pattern in line_lower for pattern in ['kgsl fault', 'kgsl hang', 'adreno gpu fault',
                                                        'gpu crash', 'kgsl recovery failed']):
                episode.corroborators.has_gpu_driver_crash = True

            # Performance subsystem abuse
            if any(pattern in line_lower for pattern in ['perf_event error', 'perf overflow',
                                                        'perf access denied untrusted']):
                episode.corroborators.has_perf_event_abuse = True

            # Futex exploitation indicators
            if any(pattern in line_lower for pattern in ['futex wait failed', 'futex wake error',
                                                        'kernel oops futex']):
                episode.corroborators.has_futex_exploitation = True

            # General kernel corruption
            if any(pattern in line_lower for pattern in ['kernel oops', 'kernel panic memory',
                                                        'kernel bug corruption']):
                episode.corroborators.has_kernel_oops = True

    def _classify_episode_type(self, episode: MemoryEpisode):
        """Classify the episode type based on evidence."""
        if episode.corroborators.has_hprof_file or episode.corroborators.has_oom_text:
            if episode.corroborators.has_fatal_signal:
                episode.episode_type = EpisodeType.EXPLOIT_CRASH
            else:
                episode.episode_type = EpisodeType.SUSPECTED_HEAP_DUMP
        else:
            episode.episode_type = EpisodeType.BACKTRACE_ONLY

    def _calculate_base_score(self, episode: MemoryEpisode, log_data: LogData) -> float:
        """Calculate base score for episode based on type and corroborators."""
        # Base score by episode type
        base_scores = {
            EpisodeType.BACKTRACE_ONLY: 1.0,
            EpisodeType.SUSPECTED_HEAP_DUMP: 6.0,
            EpisodeType.EXPLOIT_CRASH: 8.0,
        }
        
        score = base_scores.get(episode.episode_type, 1.0)
        
        # Add corroborator bonuses
        if episode.corroborators.has_hprof_file:
            score += 2.0
        if episode.corroborators.has_oom_text:
            score += 1.0
        if episode.corroborators.has_gc_thrash:
            score += 1.0
        if episode.corroborators.has_decoder_hint:
            score += 1.5  # Higher weight for zero-click targets
        if episode.corroborators.has_fatal_signal:
            score += 2.0

        # KERNEL EXPLOITATION BONUSES (2025 Android Security)
        if episode.corroborators.has_binder_corruption:
            score += 3.0  # High severity - kernel IPC corruption
        if episode.corroborators.has_gpu_driver_crash:
            score += 2.5  # High severity - driver exploitation
        if episode.corroborators.has_perf_event_abuse:
            score += 2.0  # Medium-high severity - performance subsystem abuse
        if episode.corroborators.has_futex_exploitation:
            score += 2.5  # High severity - kernel synchronization exploitation
        if episode.corroborators.has_kernel_oops:
            score += 3.5  # Critical severity - kernel memory corruption

        # High-risk process bonus
        if self._is_high_risk_process(episode.process):
            score += 2.0

        # Diagnostic dump demotion (halve score for manual dumps)
        if self._is_diagnostic_dump(episode, log_data.raw_lines, log_data):
            score *= 0.5
        
        return min(score, 10.0)

    def _apply_click_methodology(self, episode: MemoryEpisode, log_data: LogData, base_score: float) -> Tuple[float, List[str]]:
        """Apply zero-click boost or one-click dampening based on analysis."""
        methodology_evidence = []
        final_score = base_score
        
        # Analyze for zero-click indicators
        is_background_parser = self._is_background_parser_process(episode.process)
        has_foreground_activity = self._has_foreground_activity_in_zero_click_window(episode, log_data.raw_lines, log_data)
        has_memory_safety_signal = self._has_memory_safety_signal(episode)
        
        # Zero-click boost conditions:
        # 1. Background parser/sandbox process
        # 2. No foreground activity before crash
        # 3. Memory safety violation signal present
        if is_background_parser and not has_foreground_activity and has_memory_safety_signal:
            final_score = base_score * self.zero_click_boost_multiplier
            methodology_evidence.append("Zero-click exploitation pattern detected")
            methodology_evidence.append("Background parser process with memory safety violation")
            
            if episode.corroborators.has_decoder_hint:
                methodology_evidence.append("Media decoder exploitation vector")
        
        # One-click dampening conditions:
        # 1. Activity start detected before crash
        # 2. App was in foreground/topmost
        elif has_foreground_activity and self._was_app_topmost(episode, log_data.raw_lines, log_data):
            final_score = base_score * self.one_click_dampener_multiplier
            methodology_evidence.append("One-click user interaction detected before crash")
            methodology_evidence.append("Application was in foreground during crash")
        
        return min(final_score, 10.0), methodology_evidence

    def _is_background_parser_process(self, process: str) -> bool:
        """Check if process is a background parser/sandbox using multiple signals."""
        if not process:
            return False

        process_lower = process.lower()

        # Primary indicators: media processing components (high confidence)
        media_indicators = [
            'mediacodec', 'codec2', 'media.swcodec', 'cameraserver',
            'audioserver', 'media.extractor', 'media.metrics'
        ]

        # Secondary indicators: sandbox/parser processes (medium confidence)
        parser_indicators = [
            'isolated_app', 'webview', 'renderer', 'gpu-process',
            'utility', 'sandbox'
        ]

        # Check for media processing (high confidence background)
        if any(indicator in process_lower for indicator in media_indicators):
            return True

        # Check for parser processes (medium confidence, needs UI check)
        if any(indicator in process_lower for indicator in parser_indicators):
            return True

        # Check for kernel-level processes (high confidence zero-click targets)
        if any(kernel_proc in process_lower for kernel_proc in self.KERNEL_LEVEL_PROCESSES):
            return True

        # Fallback to original list for compatibility
        return any(parser in process_lower for parser in self.BACKGROUND_PARSER_PROCESSES)

    def _is_high_risk_process(self, process: str) -> bool:
        """Check if process is high-risk for memory exploitation."""
        if not process:
            return False
        
        process_lower = process.lower()
        return any(risk_proc in process_lower for risk_proc in self.HIGH_RISK_PROCESSES)

    def _has_foreground_activity_in_zero_click_window(self, episode: MemoryEpisode, all_lines: List[str], log_data: LogData) -> bool:
        """Check for foreground activity in zero-click window [-5s, +3s] around crash."""
        if not episode.start_time:
            return False

        # Zero-click window: [-5s, +3s] as per spec
        window_start = episode.start_time - timedelta(seconds=self.zero_click_window_before)
        window_end = episode.start_time + timedelta(seconds=self.zero_click_window_after)

        for i, line in enumerate(all_lines):
            timestamp = self._extract_timestamp(line, i, log_data)
            if not timestamp:
                continue

            if window_start <= timestamp <= window_end:
                # Check for activity start patterns
                if any(pattern.search(line) for pattern in self.ACTIVITY_START_PATTERNS):
                    return True
                # Check for user interaction patterns
                if any(pattern.search(line) for pattern in self.USER_INTERACTION_PATTERNS):
                    return True

        return False

    def _has_memory_safety_signal(self, episode: MemoryEpisode) -> bool:
        """Check for memory safety violation signals in episode."""
        for line in episode.raw_lines:
            # Check for strong memory safety signals
            if any(pattern.search(line) for pattern in self.STRONG_MEMORY_SIGNALS):
                return True
            # Check for weak signals with decoder context
            if (any(pattern.search(line) for pattern in self.WEAK_MEMORY_SIGNALS) and
                episode.corroborators.has_decoder_hint):
                return True
        
        return False

    def _was_app_topmost(self, episode: MemoryEpisode, all_lines: List[str], log_data: LogData) -> bool:
        """Check if app was topmost/foreground using canonical AM/WM lines."""
        if not episode.process or not episode.start_time:
            return False

        # Extract package name from process (e.g., "com.example.app:service" -> "com.example.app")
        package_name = episode.process.split(':')[0] if ':' in episode.process else episode.process

        # Canonical patterns for foreground detection
        foreground_patterns = [
            # ActivityTaskManager/ActivityManager START
            re.compile(rf'ActivityTaskManager.*START.*cmp={re.escape(package_name)}/', re.IGNORECASE),
            re.compile(rf'ActivityManager.*START.*cmp={re.escape(package_name)}/', re.IGNORECASE),

            # Displayed activity
            re.compile(rf'Displayed {re.escape(package_name)}/', re.IGNORECASE),

            # WindowManager focus
            re.compile(rf'WindowManager.*Current focus=Window{{.*{re.escape(package_name)}/', re.IGNORECASE),

            # Top resumed activity
            re.compile(rf'topResumedActivity={re.escape(package_name)}/', re.IGNORECASE),
        ]

        # Look for canonical foreground indicators within crash window
        for i, line in enumerate(all_lines):
            timestamp = self._extract_timestamp(line, i, log_data)
            if not timestamp:
                continue

            # Check within crash window (±5 seconds)
            if abs((timestamp - episode.start_time).total_seconds()) <= 5:
                if any(pattern.search(line) for pattern in foreground_patterns):
                    return True

        return False

    def _has_multiple_indicators(self, episode: MemoryEpisode) -> bool:
        """Check if episode has multiple independent indicators to reduce false positives."""
        indicators = 0

        # Memory safety signals (treat as one indicator to avoid double-counting)
        has_memory_safety = self._has_memory_safety_signal(episode)
        if has_memory_safety or episode.corroborators.has_fatal_signal:
            indicators += 1

        # Decoder/media processing hint
        if episode.corroborators.has_decoder_hint:
            indicators += 1

        # Memory pressure indicators
        if episode.corroborators.has_oom_text:
            indicators += 1

        # Heap dump evidence (but not manual dumps)
        if episode.corroborators.has_hprof_file:
            indicators += 1

        # High-risk process
        if self._is_high_risk_process(episode.process):
            indicators += 1

        # GC thrashing
        if episode.corroborators.has_gc_thrash:
            indicators += 1

        # KERNEL EXPLOITATION INDICATORS (2025 Android Security)
        kernel_indicators = 0
        if episode.corroborators.has_binder_corruption:
            kernel_indicators += 1
        if episode.corroborators.has_gpu_driver_crash:
            kernel_indicators += 1
        if episode.corroborators.has_perf_event_abuse:
            kernel_indicators += 1
        if episode.corroborators.has_futex_exploitation:
            kernel_indicators += 1
        if episode.corroborators.has_kernel_oops:
            kernel_indicators += 1

        # Count kernel indicators as one bucket to avoid double counting
        if kernel_indicators > 0:
            indicators += 1

        # Require at least 2 independent indicators for high-confidence detection
        return indicators >= 2

    def _create_episode_detection(self, episode: MemoryEpisode, score: float, methodology_evidence: List[str]) -> Detection:
        """Create a Detection object from a memory episode."""
        # Determine severity based on score and episode type
        if score >= 8.0 or episode.episode_type == EpisodeType.EXPLOIT_CRASH:
            severity = Severity.CRITICAL
        elif score >= 6.0 or episode.episode_type == EpisodeType.SUSPECTED_HEAP_DUMP:
            severity = Severity.HIGH
        else:
            severity = Severity.MEDIUM
        
        # Create title and description
        process_name = episode.process or "unknown"
        episode_type_name = episode.episode_type.value.replace('_', ' ').title()
        
        title = f"Memory Exploitation Episode: {process_name}"
        description = f"{episode_type_name} detected in {process_name} (Score: {score:.1f})"
        
        # Collect corroborator evidence
        corroborator_evidence = []
        if episode.corroborators.has_hprof_file:
            corroborator_evidence.append("HPROF file detected")
        if episode.corroborators.has_oom_text:
            corroborator_evidence.append("Out of Memory error")
        if episode.corroborators.has_gc_thrash:
            corroborator_evidence.append("GC thrashing detected")
        if episode.corroborators.has_decoder_hint:
            corroborator_evidence.append("Media decoder involvement")
        if episode.corroborators.has_fatal_signal:
            corroborator_evidence.append("Fatal signal detected")
        
        # Create evidence objects - select most probative lines
        evidence = []
        probative_lines = self._select_probative_evidence(episode)
        for line in probative_lines[:5]:  # Limit to 5 most important lines
            evidence.append(Evidence(
                type=EvidenceType.LOG_ANCHOR,
                content=line,
                confidence=0.8
            ))
        
        return Detection(
            category=self.category,
            package=process_name,
            severity=severity,
            confidence=min(score / 10.0, 1.0),
            title=title,
            description=description,
            technical_details={
                'episode_type': episode.episode_type.value,
                'process_name': process_name,
                'pid': episode.pid,
                'score': score,
                'corroborators': corroborator_evidence,
                'methodology_evidence': methodology_evidence,
                'start_time': episode.start_time.isoformat() if episode.start_time else None,
                'end_time': episode.end_time.isoformat() if episode.end_time else None,
                'abuse_type': 'memory_exploitation'
            },
            evidence=evidence
        )

    def _extract_timestamp(self, line: str, line_index: int = 0, log_data=None) -> Optional[datetime]:
        """Extract timestamp from log line supporting multiple formats with caching and relative timestamp support."""
        # Create cache key that includes log_data availability to prevent wrong reuse
        has_log_data = log_data is not None
        cache_key = (line_index, line, has_log_data)
        if cache_key in self._timestamp_cache:
            return self._timestamp_cache[cache_key]

        from datetime import datetime

        timestamp = None

        # Android logcat format: MM-DD HH:MM:SS.mmm
        logcat_match = re.search(r'(\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}\.\d{3})', line)
        if logcat_match:
            try:
                date_part = logcat_match.group(1)
                time_part = logcat_match.group(2)

                # Use current year (logcat doesn't include year)
                current_year = datetime.now().year
                timestamp_str = f"{current_year}-{date_part} {time_part}"
                timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f")
            except ValueError:
                pass

        # Support relative timestamps from dmesg/audit: [ 12345.678 ]
        if not timestamp:
            relative_match = re.search(r'\[\s*(\d+\.\d+)\s*\]', line)
            if relative_match:
                try:
                    seconds_since_boot = float(relative_match.group(1))

                    # Convert to absolute time if boot_time is available
                    if log_data and hasattr(log_data, 'boot_time') and log_data.boot_time:
                        timestamp = log_data.boot_time + timedelta(seconds=seconds_since_boot)
                    else:
                        # Create relative timestamp for proper grouping
                        # Use epoch + seconds for consistent relative comparison
                        epoch = datetime(1970, 1, 1)
                        timestamp = epoch + timedelta(seconds=seconds_since_boot)
                except (ValueError, TypeError):
                    pass

        # Cache the result
        self._timestamp_cache[cache_key] = timestamp
        return timestamp

    def _select_probative_evidence(self, episode: MemoryEpisode) -> List[str]:
        """Select the most probative lines for evidence (strong signal → decoder lib → OOM/HPROF)."""
        lines_with_scores = []

        for line in episode.raw_lines:
            score = 0
            line_lower = line.lower()

            # Highest priority: Strong memory corruption signals
            if any(pattern.search(line) for pattern in self.STRONG_MEMORY_SIGNALS):
                score += 10

            # High priority: Decoder/media library involvement
            if any(decoder in line_lower for decoder in ['stagefright', 'mediacodec', 'libwebp', 'skia']):
                score += 8

            # Medium-high priority: HPROF/heap dumps
            if any(pattern in line_lower for pattern in ['hprof', 'heap dump', 'dumpheap']):
                score += 6

            # Medium priority: OOM indicators
            if any(pattern in line_lower for pattern in ['outofmemoryerror', 'out of memory', 'oom']):
                score += 5

            # Lower priority: General crash indicators
            if any(pattern in line_lower for pattern in ['fatal signal', 'sigsegv', 'sigbus']):
                score += 3

            # Kernel exploitation indicators
            if any(pattern.search(line) for pattern in self.KERNEL_PATTERNS):
                score += 7

            lines_with_scores.append((score, line))

        # Sort by score (highest first) and return lines
        lines_with_scores.sort(key=lambda x: x[0], reverse=True)
        return [line for _, line in lines_with_scores]

    def _is_diagnostic_dump(self, episode: MemoryEpisode, all_lines: List[str], log_data: LogData) -> bool:
        """Check if HPROF dump is diagnostic (manual) by scanning ±1s neighborhood."""
        if not episode.start_time:
            return False

        # Diagnostic dump indicators
        diagnostic_tokens = [
            'am dumpheap',
            'Debug.dumpHprofData',
            'hprof-conv',
            'manual dump',
            'diagnostic dump'
        ]

        # Check ±1 second window around episode start
        window_start = episode.start_time - timedelta(seconds=1)
        window_end = episode.start_time + timedelta(seconds=1)

        for i, line in enumerate(all_lines):
            timestamp = self._extract_timestamp(line, i, log_data)
            if not timestamp:
                continue

            if window_start <= timestamp <= window_end:
                line_lower = line.lower()
                if any(token in line_lower for token in diagnostic_tokens):
                    return True

        return False

    def _extract_process_name(self, line: str) -> Optional[str]:
        """Extract process name from log line."""
        # Look for common process name patterns
        patterns = [
            re.compile(r'Process\s+([^\s\(]+)', re.IGNORECASE),
            re.compile(r'PID:\s*\d+.*?([a-zA-Z][a-zA-Z0-9_.]+)', re.IGNORECASE),
            re.compile(r'([a-zA-Z][a-zA-Z0-9_.]+):\s+', re.IGNORECASE),
        ]
        
        for pattern in patterns:
            match = pattern.search(line)
            if match:
                process = match.group(1)
                # Filter out common non-process strings
                if len(process) > 2 and not process.isdigit():
                    return process
        
        return None

    def _extract_pid(self, line: str) -> Optional[str]:
        """Extract process ID from log line."""
        pid_pattern = re.compile(r'pid[:\s]+(\d+)', re.IGNORECASE)
        match = pid_pattern.search(line)
        return match.group(1) if match else None



